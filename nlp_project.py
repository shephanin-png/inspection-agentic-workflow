# -*- coding: utf-8 -*-
"""NLP_Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tQCh-jtpZGMGlAIn1wRwaFiVmxtXhV8r
"""

# ëŸ°íƒ€ì„ í™•ì¸
!nvidia-smi || true
!cat /etc/os-release | head
!python --version

# Ollama ì„¤ì¹˜
!sudo apt-get update && sudo apt-get install -y zstd
!curl -fsSL https://ollama.com/install.sh | sh

# ì„¤ì¹˜ í™•ì¸
!ollama --version

# Ollama ì„œë²„ ì‹¤í–‰
import subprocess, time, textwrap, os, signal

# Ollama server start
proc = subprocess.Popen(["ollama", "serve"], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)

# ì„œë²„ê°€ ëœ° ì‹œê°„ì„ ì¡°ê¸ˆ ì¤Œ
time.sleep(2)

print("Ollama server started (pid):", proc.pid)

# ì„œë²„ í™•ì¸
!curl -s http://127.0.0.1:11434/api/tags | head -c 500

# ëª¨ë¸ pull
!ollama pull llama3

import requests, json

def ollama_chat(prompt: str, model: str="llama3") -> str:
    url = "http://127.0.0.1:11434/api/chat"
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": "You are a helpful assistant for inspection process troubleshooting."},
            {"role": "user", "content": prompt}
        ],
        "stream": False,
        "options": {
            "temperature": 0.2
        }
    }
    r = requests.post(url, json=payload, timeout=600) # Increased timeout to 600 seconds
    r.raise_for_status()
    return r.json()["message"]["content"]

test_prompt = "swp ì—ëŸ¬ ë¡œê·¸ ë°œìƒí–ˆìŠµë‹ˆë‹¤. íŠ¸ëŸ¬ë¸” ìŠˆíŒ… ê°€ì´ë“œ ê°„ë‹¨íˆ 3ì¤„ë¡œ."
print(ollama_chat(test_prompt, model="llama3"))

# ë°ì´í„° ì¤€ë¹„
import pandas as pd
import numpy as np

np.random.seed(42)

# ----- Constants (ì›í•˜ëŠ” ê°’ìœ¼ë¡œ ìˆ˜ì • ê°€ëŠ¥) -----
EQPID = "KR-010101"
MODEL = "LGMONYTV"
Program_Version = "260206"
Equipment_Version = "M.KOR"
Spec_Min_mA = 0.80
Spec_Max_mA = 10.00

def make_row(i, label, s6f1_result, saved_result, delta_s, acw):
    return {
        "issue_id": f"{i:03d}",
        "label": label,  # OK / NG (ì •ë‹µ ë¼ë²¨)
        "EQPID": EQPID,
        "SETID": f"500MONY{i:04d}",
        "MODEL": MODEL,
        "Program_Version": Program_Version,
        "Equipment_Version": Equipment_Version,
        "S6F12_ACK": 0,
        "delta_S6F11_S6F12_s": int(delta_s),
        "S6F1_RESULT": s6f1_result,     # ì¥ë¹„ íŒë‹¨
        "ACW_Mea_mA": round(float(acw), 2),
        "Spec_Min_mA": Spec_Min_mA,
        "Spec_Max_mA": Spec_Max_mA,
        "Saved_Result": saved_result,   # ì €ì¥ ê²°ê³¼
    }

rows = []

# ----- OK ë°ì´í„° 50ê°œ (001~050): RESULT=OK, Saved=OK -----
ok_acw = np.clip(
    np.random.normal(loc=4.5, scale=2.0, size=50),
    Spec_Min_mA + 0.01, Spec_Max_mA - 0.01
)
ok_delta = np.random.choice([1, 2, 3], size=50, p=[0.6, 0.3, 0.1])

for idx in range(1, 51):
    rows.append(make_row(idx, "OK", "OK", "OK", ok_delta[idx-1], ok_acw[idx-1]))

# ----- NG ë°ì´í„° 10ê°œ (051~060)
#  - mismatch 6ê°œ: S6F1 OK but Saved NG
#  - true NG 4ê°œ: S6F1 NG and Saved NG
ng_plan = {
    51: ("NG", "OK", "NG", 1, 3.10),
    52: ("NG", "OK", "NG", 2, 4.62),
    53: ("NG", "NG", "NG", 1, 2.40),
    54: ("NG", "OK", "NG", 3, 9.12),
    55: ("NG", "NG", "NG", 1, 6.07),
    56: ("NG", "OK", "NG", 2, 2.88),
    57: ("NG", "NG", "NG", 1, 1.05),
    58: ("NG", "OK", "NG", 2, 7.41),
    59: ("NG", "NG", "NG", 1, 3.77),
    60: ("NG", "OK", "NG", 2, 5.18),
}

for idx in range(51, 61):
    label, s6f1_result, saved_result, delta_s, acw = ng_plan[idx]
    # ìŠ¤í™ì€ ì •ìƒ ì°íˆëŠ” ì¡°ê±´ ìœ ì§€(ìŠ¤í™ ë²”ìœ„ ë‚´ë¡œ í´ë¦½)
    acw = min(max(acw, Spec_Min_mA + 0.01), Spec_Max_mA - 0.01)
    rows.append(make_row(idx, label, s6f1_result, saved_result, delta_s, acw))

df = pd.DataFrame(rows)

# (ì„ íƒ) íŒŒìƒ ì»¬ëŸ¼: mismatch ì—¬ë¶€ / ì¼€ì´ìŠ¤ íƒ€ì…
df["is_mismatch"] = ((df["S6F1_RESULT"] != df["Saved_Result"]) & (df["label"] == "NG")).astype(int)
df["case_type"] = np.where(
    df["label"] == "OK",
    "ok_normal",
    np.where(df["is_mismatch"] == 1, "mismatch_ok_saved_ng", "true_ng_s6f1_ng")
)

# ----- CSV ì €ì¥ -----
out_path = "/content/inspection_logs_60.csv"
df.to_csv(out_path, index=False, encoding="utf-8-sig")

print("Saved:", out_path)
df.head(10)

!pip -q install langchain langchain-community chromadb pypdf sentence-transformers

from langchain_community.document_loaders import PyPDFLoader
from google.colab import drive
drive.mount('/content/drive')
pdf_path = "/content/drive/MyDrive/[GW_INSTEK]GPT-9800-SERIES_USER-MANUAL_KR.pdf"
loader = PyPDFLoader(pdf_path)

documents = loader.load()
print("Total pages:", len(documents))

# í…ìŠ¤íŠ¸ ë¶„í• 
!pip install -U langchain langchain-community langchain-text-splitters
from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=100,
    separators=["\n\n", "\n", " ", ""]
)

splits = splitter.split_documents(documents)

print("Total chunks:", len(splits))
print(splits[0].page_content[:500])

# ì„ë² ë”© + ë²¡í„° DB ìƒì„±
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings,
    persist_directory="/content/chroma_gpt9800_manual"
)

print("Vector DB ready:", vectorstore._collection.count())

# RAG + Ollama (Vanilla + RAG Baseline)
from langchain_community.chat_models import ChatOllama

llm = ChatOllama(model="llama3", temperature=0.2)

def rag_answer_from_manual(question: str, k: int = 4):
    retriever = vectorstore.as_retriever(search_kwargs={"k": k})
    docs = retriever.invoke(question)   # ğŸ‘ˆ ì—¬ê¸°

    context = "\n\n".join(
        [f"[MANUAL]\n{d.page_content}" for d in docs]
    )

    prompt = f"""
ë„ˆëŠ” GPT-9800 ê²€ì‚¬ ì¥ë¹„ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì „ë¬¸ê°€ì•¼.
ì•„ë˜ MANUAL ë‚´ìš©ë§Œ ê·¼ê±°ë¡œ ë‹µë³€í•´.

MANUAL:
{context}

QUESTION:
{question}

ìš”êµ¬ì‚¬í•­:
- ê°€ëŠ¥í•œ ì›ì¸ 2~3ê°œ
- í™•ì¸ ì ˆì°¨(ë¡œê·¸/S6F ì‹œí€€ìŠ¤ ì¤‘ì‹¬) 5ë‹¨ê³„ ì´ë‚´
- ì„ì‹œ ì¡°ì¹˜ 2ê°œ
"""
    return llm.invoke(prompt).content

q = "ê²€ì‚¬ í”„ë¡œê·¸ë¨ì—ì„œ ìŠ¤ìº”ì„ í–ˆëŠ”ë° ê²€ì‚¬ ì§„í–‰ì´ ì•ˆë¼. ì›ì¸ê³¼ ì¡°ì¹˜ ë°©ë²• ì•Œë ¤ì¤˜"
print(rag_answer_from_manual(q))

from langchain.tools import tool
import pandas as pd

@tool
def manual_rag_tool(question: str) -> str:
    """
    GPT-9800 ê²€ì‚¬ ì¥ë¹„ ë§¤ë‰´ì–¼ ê¸°ë°˜ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… Tool.
    ë§¤ë‰´ì–¼ ê·¼ê±°ë¡œë§Œ ì›ì¸, í™•ì¸ ì ˆì°¨, ì„ì‹œ ì¡°ì¹˜ë¥¼ ì œì‹œí•œë‹¤.
    """

    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
    docs = retriever.invoke(question)

    if not docs:
        return "ë§¤ë‰´ì–¼ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    context = "\n\n".join(
        [f"[MANUAL]\n{d.page_content}" for d in docs]
    )

    prompt = f"""
ë„ˆëŠ” GPT-9800 ê²€ì‚¬ ì¥ë¹„ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì „ë¬¸ê°€ì•¼.
ì•„ë˜ MANUAL ë‚´ìš©ë§Œ ê·¼ê±°ë¡œ ë‹µë³€í•´.

MANUAL:
{context}

QUESTION:
{question}

ìš”êµ¬ì‚¬í•­:
- ê°€ëŠ¥í•œ ì›ì¸ 2~3ê°œ
- í™•ì¸ ì ˆì°¨(ë¡œê·¸/S6F ì‹œí€€ìŠ¤ ì¤‘ì‹¬) 5ë‹¨ê³„ ì´ë‚´
- ì„ì‹œ ì¡°ì¹˜ 2ê°œ
"""

    return llm.invoke(prompt).content

from langchain.tools import tool

@tool
def inspection_log_tool(_: str) -> str:
    """
    ë©”ëª¨ë¦¬ì— ì¡´ì¬í•˜ëŠ” inspection log DataFrame(df) ê¸°ë°˜ ë¶„ì„ Tool
    """

    total = len(df)
    ng_cnt = (df["label"] == "NG").sum()
    mismatch_cnt = df["is_mismatch"].sum()

    sample = df[df["is_mismatch"] == 1][
        ["issue_id", "S6F1_RESULT", "Saved_Result", "ACW_Mea_mA"]
    ].head(5)

    result = f"""
[Inspection Summary]
- ì´ ê²€ì‚¬ ìˆ˜ëŸ‰: {total}
- NG ìˆ˜ëŸ‰: {ng_cnt}
- Mismatch ìˆ˜ëŸ‰: {mismatch_cnt}

[Sample Mismatch Cases]
{sample.to_string(index=False)}
"""
    return result

from langchain_community.chat_models import ChatOllama

agent_llm = ChatOllama(
    model="llama3",
    temperature=0
)

!pip install --upgrade --force-reinstall langchain langchain-community

from langchain.agents import initialize_agent, AgentType
from langchain_core.messages import SystemMessage

tools = [manual_rag_tool, inspection_log_tool]

SYSTEM = SystemMessage(content=
    "ë„ˆëŠ” GPT-9800 íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì—ì´ì „íŠ¸ë‹¤. "
    "ë°˜ë“œì‹œ toolsë¥¼ ì‚¬ìš©í•´ì„œ ê·¼ê±°ë¥¼ í™•ë³´í•œ ë’¤ ë‹µí•˜ë¼. "
    "manual_rag_tool ë˜ëŠ” inspection_log_tool ì¤‘ ìµœì†Œ 1ê°œëŠ” ë°˜ë“œì‹œ í˜¸ì¶œí•˜ë¼. "
    "ë§¤ë‰´ì–¼ ê·¼ê±°ê°€ ì—†ìœ¼ë©´ 'ë§¤ë‰´ì–¼ ê·¼ê±°ë¡œ í™•ì¸ ë¶ˆê°€'ë¼ê³  ë‹µí•˜ë¼."
)

react_executor = initialize_agent(
    tools=tools,
    llm=agent_llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
    agent_kwargs={"system_message": SYSTEM}
)

q = "ê²€ì‚¬ í”„ë¡œê·¸ë¨ì—ì„œ ìŠ¤ìº”ì„ í–ˆëŠ”ë° ê²€ì‚¬ ì§„í–‰ì´ ì•ˆë¼. ì›ì¸ê³¼ ì¡°ì¹˜ ë°©ë²• ì•Œë ¤ì¤˜"
print(react_executor.run(q))



