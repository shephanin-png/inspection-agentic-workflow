# -*- coding: utf-8 -*-
"""NLP_Project_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ko1I7GXVEnD7yNWRJ27tqEY9kuUGV3Ag
"""

# 0) í™˜ê²½ í™•ì¸
# ----------------------------
!nvidia-smi || true
!cat /etc/os-release | head -n 3
!python --version

# 1) íŒ¨í‚¤ì§€ ì„¤ì¹˜
#    - langchain / langgraph / chroma / pymupdf
# ----------------------------
!pip -q install -U \
  langchain langchain-core langchain-community \
  langchain-ollama langchain-chroma \
  langgraph \
  pymupdf \
  pydantic \
  datasets ragas

# 2) Ollama ì„¤ì¹˜ + ì„œë²„ ê¸°ë™ + ëª¨ë¸ pull (ì—ëŸ¬ ë°©ì§€)
# ----------------------------
import os, time, subprocess, json, urllib.request

# zstd ì„¤ì¹˜ (ollama ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ìš”êµ¬í•  ìˆ˜ ìˆìŒ)
!sudo apt-get update -y >/dev/null
!sudo apt-get install -y zstd curl >/dev/null

# ollama ì„¤ì¹˜
!curl -fsSL https://ollama.com/install.sh | sh

def is_ollama_up(url="http://127.0.0.1:11434/api/tags", timeout=2) -> bool:
    try:
        with urllib.request.urlopen(url, timeout=timeout) as resp:
            _ = resp.read(200)
        return True
    except Exception:
        return False

# ì„œë²„ê°€ ì´ë¯¸ ë– ìˆìœ¼ë©´ ë‹¤ì‹œ ë„ìš°ì§€ ì•ŠìŒ
if not is_ollama_up():
    # ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰
    _ = subprocess.Popen(["ollama", "serve"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    time.sleep(2)

# ìµœì¢… í™•ì¸ (ì—¬ê¸°ì„œ ì‹¤íŒ¨í•˜ë©´ ì´í›„ê°€ ë‹¤ ì•ˆë¨)
assert is_ollama_up(), "âŒ Ollama serverê°€ ë– ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. 'ollama serve'ê°€ ì‹¤íŒ¨í–ˆì–´ìš”."

# ëª¨ë¸ pull (ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ë°›ìŒ)
!ollama pull llama3
!ollama pull nomic-embed-text

print("âœ… Ollama ready")

# 3) Google Drive ë§ˆìš´íŠ¸ + PDF ë¡œë“œ (ê²½ë¡œ/íŒŒì¼ ì²´í¬ í¬í•¨)
# ----------------------------
!pip install pypdf
from google.colab import drive
from langchain_community.document_loaders import PyPDFLoader

drive.mount("/content/drive")
pdf_path = "/content/drive/MyDrive/[GW_INSTEK]GPT-9800-SERIES_USER-MANUAL_KR.pdf"
loader = PyPDFLoader(pdf_path)

if not os.path.exists(pdf_path):
    raise FileNotFoundError(
        f"âŒ PDFë¥¼ ëª» ì°¾ìŒ: {pdf_path}\n"
        "Drive ê²½ë¡œ/íŒŒì¼ëª…ì„ í™•ì¸í•˜ê³  PDF_PATHë¥¼ ìˆ˜ì •í•˜ì„¸ìš”."
    )


loader = PyPDFLoader(pdf_path)
documents = loader.load()
print("âœ… Manual pages:", len(documents))
assert len(documents) > 0, "âŒ PDFì—ì„œ í˜ì´ì§€ë¥¼ ëª» ì½ì—ˆìŠµë‹ˆë‹¤. (ì•”í˜¸í™”/ì†ìƒ ê°€ëŠ¥)"

# ë°ì´í„° ì¤€ë¹„
import pandas as pd
import numpy as np

np.random.seed(42)

# ----- Constants -----
EQPID = "KR-010101"
MODEL = "LGMONYTV"
Program_Version = "260206"
Equipment_Version = "M.KOR"
Spec_Min_mA = 0.80
Spec_Max_mA = 10.00

def make_row(i, label, s6f1_result, saved_result, delta_s, acw):
    return {
        "issue_id": f"{i:03d}",
        "label": label,  # OK / NG (ì •ë‹µ ë¼ë²¨)
        "EQPID": EQPID,
        "SETID": f"500MONY{i:04d}",
        "MODEL": MODEL,
        "Program_Version": Program_Version,
        "Equipment_Version": Equipment_Version,
        "S6F12_ACK": 0,
        "delta_S6F11_S6F12_s": int(delta_s),
        "S6F1_RESULT": s6f1_result,     # ì¥ë¹„ íŒë‹¨
        "ACW_Mea_mA": round(float(acw), 2),
        "Spec_Min_mA": Spec_Min_mA,
        "Spec_Max_mA": Spec_Max_mA,
        "Saved_Result": saved_result,   # ì €ì¥ ê²°ê³¼
    }

rows = []

# ----- OK ë°ì´í„° 100ê°œ (001~100): RESULT=OK, Saved=OK -----
ok_acw = np.clip(
    np.random.normal(loc=4.5, scale=2.0, size=100),
    Spec_Min_mA + 0.01, Spec_Max_mA - 0.01
)
ok_delta = np.random.choice([1, 2, 3], size=100, p=[0.6, 0.3, 0.1])

for idx in range(1, 101):
    rows.append(make_row(idx, "OK", "OK", "OK", ok_delta[idx-1], ok_acw[idx-1]))

# ----- NG ë°ì´í„° 10ê°œ (101~110)
#  - mismatch 6ê°œ: S6F1 OK but Saved NG
#  - true NG 4ê°œ: S6F1 NG and Saved NG
ng_plan = {
    101: ("NG", "OK", "NG", 1, 3.10),
    102: ("NG", "OK", "NG", 2, 4.62),
    103: ("NG", "NG", "NG", 1, 2.40),
    104: ("NG", "OK", "NG", 3, 9.12),
    105: ("NG", "NG", "NG", 1, 6.07),
    106: ("NG", "OK", "NG", 2, 2.88),
    107: ("NG", "NG", "NG", 1, 1.05),
    108: ("NG", "OK", "NG", 2, 7.41),
    109: ("NG", "NG", "NG", 1, 3.77),
    110: ("NG", "OK", "NG", 2, 5.18),
}

for idx in range(101, 111):
    label, s6f1_result, saved_result, delta_s, acw = ng_plan[idx]
    # ìŠ¤í™ì€ ì •ìƒ ì°íˆëŠ” ì¡°ê±´ ìœ ì§€(ìŠ¤í™ ë²”ìœ„ ë‚´ë¡œ í´ë¦½)
    acw = min(max(acw, Spec_Min_mA + 0.01), Spec_Max_mA - 0.01)
    rows.append(make_row(idx, label, s6f1_result, saved_result, delta_s, acw))

df = pd.DataFrame(rows)

# íŒŒìƒ ì»¬ëŸ¼: mismatch ì—¬ë¶€ / ì¼€ì´ìŠ¤ íƒ€ì…
df["is_mismatch"] = ((df["S6F1_RESULT"] != df["Saved_Result"]) & (df["label"] == "NG")).astype(int)
df["case_type"] = np.where(
    df["label"] == "OK",
    "ok_normal",
    np.where(df["is_mismatch"] == 1, "mismatch_ok_saved_ng", "true_ng_s6f1_ng")
)

# ----- CSV ì €ì¥ -----
out_path = "/content/inspection_logs_110.csv"
df.to_csv(out_path, index=False, encoding="utf-8-sig")

print("Saved:", out_path)
df.head(10)

# 5) Split + Vectorstore (Chroma) - Ollama Embeddings ì‚¬ìš©
# ----------------------------
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=100,
    separators=["\n\n", "\n", " ", ""]
)
splits = splitter.split_documents(documents)
print("âœ… chunks:", len(splits))
assert len(splits) > 0, "âŒ split ê²°ê³¼ê°€ 0ì…ë‹ˆë‹¤."


embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings,
    persist_directory="/content/chroma_gpt9800_manual"
)

print("Vector DB ready:", vectorstore._collection.count())

from langchain_community.chat_models import ChatOllama

# Baselineì€ llama3
llm_baseline = ChatOllama(model="llama3", temperature=0.2)

# Agenticì€ tools ì§€ì› ëª¨ë¸
llm_agent = ChatOllama(model="llama3.1:8b", temperature=0.2)

# 6) Baseline (Vanilla + RAG chain)
# =========================
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.documents import Document
from typing import List


llm = ChatOllama(model="llama3", temperature=0.2)

retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

def format_docs(docs: List[Document]) -> str:
    blocks = []
    for d in docs:
        src = d.metadata.get("source", "manual_pdf")
        page = d.metadata.get("page", None)
        page_str = f"page={page}" if page is not None else ""
        blocks.append(f"[manual] source={src} {page_str}\n{d.page_content}")
    return "\n\n---\n\n".join(blocks)

baseline_template = """ë„ˆëŠ” GPT-9804 ê²€ì‚¬ ì¥ë¹„ ì „ë¬¸ê°€ì•¼.
MANUAL(pdfíŒŒì¼) ë‚´ìš©ì— ê·¼ê±°í•´ì„œë§Œ ë‹µë³€í•´.
MANUALì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ë‹¨ì •í•´ì„œ ë§í•˜ì§€ ë§ê³  'ì¶”ê°€ í™•ì¸ í•„ìš”'ë¼ê³  ë§í•´.

ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥:
â–¶ ìš”ì•½
...
â–¶ ì›ì¸
...
â–¶ ì›ì¸ ë¶„ì„ evidence
1) ...
2) ...
â–¶ ì„ì‹œ ë°©ì•ˆ
...
â–¶ ì¶”ê°€ í•„ìš” ì‚¬í•­
...

MANUAL:
{context}

QUESTION:
{question}
"""

baseline_prompt = ChatPromptTemplate.from_template(baseline_template)

baseline_rag_chain = (
    {
        "context": retriever | RunnableLambda(format_docs),
        "question": RunnablePassthrough()
    }
    | baseline_prompt
    | llm_baseline
    | StrOutputParser()
)

print("âœ… Baseline RAG chain ready.")

# 7) Agentic
# =========================
from langchain.tools import tool
from langchain.tools import BaseTool
from typing import Dict, Any, List, Optional
import json, re

@tool
def manual_retrieve_tool(query: str) -> str:
    """ë§¤ë‰´ì–¼ì—ì„œ ê´€ë ¨ chunkë§Œ ê°€ì ¸ì™€ ë°˜í™˜ (ë‹µë³€ ìƒì„± X)"""
    docs = retriever.invoke(query)
    if not docs:
        return "NO_CONTEXT_FOUND"
    return "\n\n---\n\n".join(d.page_content for d in docs)

@tool
def inspection_log_tool(_: str = "") -> str:
    """df ê¸°ë°˜ mismatch/NG ìš”ì•½"""
    total = len(df)
    ng = int((df["label"] == "NG").sum())
    mismatch = int(df["is_mismatch"].sum())
    sample = df[df["is_mismatch"] == 1][
        ["issue_id", "SETID", "S6F1_RESULT", "Saved_Result", "ACW_Mea_mA"]
    ].head(5)
    return f"""
[Inspection Summary]
- Total: {total}
- NG: {ng}
- Mismatch: {mismatch}

[Sample]
{sample.to_string(index=False)}
""".strip()

@tool
def extract_log_signals_tool(text: str) -> str:
    """ë¡œê·¸ í…ìŠ¤íŠ¸ì—ì„œ ack / SxFy / LO/HI SET íŒíŠ¸ ì¶”ì¶œ(ë¬¸ìì—´ë¡œ ë°˜í™˜)"""
    ack = re.findall(r"\back\s*[:=]?\s*(\d+)\b", text, flags=re.I)
    secs = re.findall(r"\bS\d+F\d+\b", text, flags=re.I)
    params = re.findall(r"\b(LO\s*SET|HI\s*SET)\b", text, flags=re.I)
    return json.dumps({
        "ack": int(ack[-1]) if ack else None,
        "secs_messages": sorted(list(set([s.upper() for s in secs]))) if secs else [],
        "params": sorted(list(set([p.upper().replace(" ", "") for p in params]))) if params else [],
        "log_len": len(text)
    }, ensure_ascii=False)

TOOLS: Dict[str, BaseTool] = {
    "manual_retrieve_tool": manual_retrieve_tool,
    "inspection_log_tool": inspection_log_tool,
    "extract_log_signals_tool": extract_log_signals_tool,
}

def extract_json_objects(text: str) -> List[Dict[str, Any]]:
    objs = []
    for m in re.finditer(r"\{[,]?[\s\S]*?\}", text):
        s = m.group()
        try:
            obj = json.loads(s)
            if isinstance(obj, dict):
                objs.append(obj)
        except Exception:
            continue
    return objs

def extract_final(text: str) -> Optional[str]:
    objs = extract_json_objects(text)
    for obj in reversed(objs):
        if "final" in obj and isinstance(obj["final"], str):
            return obj["final"]
    return None

def extract_tool_call(text: str) -> Optional[Dict[str, str]]:
    objs = extract_json_objects(text)
    for obj in reversed(objs):
        t = obj.get("tool")
        if t in TOOLS:
            inp = obj.get("input", "")
            if not isinstance(inp, str):
                inp = str(inp)
            return {"tool": t, "input": inp}
    return None

def agentic_react(question: str, max_steps: int = 4) -> str:
    memory = ""
    used_tools = set()

    def final_answer() -> str:
        return llm.invoke(
f"""
ë„ˆëŠ” GPT-9800 ê²€ì‚¬ ì¥ë¹„ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì „ë¬¸ê°€ë‹¤.
ì•„ë˜ OBSERVATIONS(ë§¤ë‰´ì–¼ ë°œì·Œ/ë¡œê·¸ ìš”ì•½/ê²€ì‚¬ì´ë ¥)ì— ê·¼ê±°í•´ì„œë§Œ ë‹µí•˜ë¼.
OBSERVATIONSì— ê·¼ê±°ê°€ ì—†ìœ¼ë©´ ë°˜ë“œì‹œ 'ì¶”ê°€ í™•ì¸ í•„ìš”'ë¡œ ë‚¨ê²¨ë¼.

OBSERVATIONS:
{memory}

USER QUESTION:
{question}

ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥:
â–¶ ìš”ì•½
...
â–¶ ì›ì¸
...
â–¶ ì›ì¸ ë¶„ì„ evidence
1) ...
2) ...
â–¶ ì„ì‹œ ë°©ì•ˆ
...
â–¶ ì¶”ê°€ í•„ìš” ì‚¬í•­
...
"""
        ).content

    for _ in range(max_steps):
        prompt = f"""
ë„ˆëŠ” GPT-9800 íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì—ì´ì „íŠ¸ë‹¤. ë°˜ë“œì‹œ ì•„ë˜ ìˆœì„œë¡œ ì§„í–‰í•´ë¼:

1) extract_log_signals_toolë¡œ ACK/SxFy ì‹ í˜¸ ì¶”ì¶œ
2) ì¶”ì¶œ ì‹ í˜¸ë¥¼ í¬í•¨í•´ manual_retrieve_tool ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì¬ì‘ì„±í•´ì„œ ë§¤ë‰´ì–¼ ê·¼ê±°ë¥¼ í™•ë³´
3) í•„ìš” ì‹œ inspection_log_toolë¡œ ì´ë ¥(mismatch) í™•ì¸
4) ê·¼ê±°ê°€ ì¶©ë¶„í•˜ë©´ {{\"final\":\"READY\"}} ë¥¼ ì¶œë ¥í•´ë¼ (READYë©´ ì‹œìŠ¤í…œì´ ìµœì¢…ë‹µì„ ìƒì„±í•œë‹¤)

OBSERVATIONS:
{memory}

QUESTION:
{question}

ê°€ëŠ¥í•œ ì¶œë ¥(JSON í•˜ë‚˜):
{{\"tool\":\"extract_log_signals_tool\",\"input\":\"ì§ˆë¬¸ì—ì„œ ë¡œê·¸/ACK/S6F í¬í•¨ ë¶€ë¶„\"}}
{{\"tool\":\"manual_retrieve_tool\",\"input\":\"ACK/S6F í¬í•¨ ì¬ì‘ì„± ì¿¼ë¦¬\"}}
{{\"tool\":\"inspection_log_tool\",\"input\":\"\"}}
{{\"final\":\"READY\"}}
"""
        resp = llm.invoke(prompt).content

        fin = extract_final(resp)
        if fin is not None:
            return final_answer()

        call = extract_tool_call(resp)
        if call is None:
            return final_answer()

        tname, tin = call["tool"], call["input"]
        if tname in used_tools:
            return final_answer()

        used_tools.add(tname)
        obs = TOOLS[tname].invoke(tin)
        memory += f"\n[{tname}]\n{obs}\n"

    return final_answer()

print("âœ… Agentic(JSON-ReAct) ready")

# 8) ë¹„êµ ì‹¤í–‰ (Baseline vs Agentic)
# =========================
q1 = "[í˜„ìƒ]SETID ìŠ¤ìº”ì€ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìœ¼ë‚˜ ê²€ì‚¬ ë™ì‘ì´ ë˜ì§€ ì•ŠìŒ. [ë¡œê·¸]10:21:14  S6F11  SETID=500MONY0051, 10:21:15  S6F12  ACK=1 [ìš´ì˜ì¡°ê±´] Program Version: 260206, Equipment Version: M.KOR, ë™ì¼ SETIDë¡œ ì¬ì‹œë„ ì‹œ ë™ì¼ í˜„ìƒ ë°œìƒ [ì§ˆë¬¸] ë¡œê·¸ ê¸°ì¤€ìœ¼ë¡œ S6F12 ACK=1 ì´ ì™œ ë°œìƒí–ˆëŠ”ì§€, í™•ì¸í•´ì•¼í•  ë§¤ë‰´ì–¼ í•­ëª© ë° ì ê²€ ì ˆì°¨ë¥¼ ì•Œë ¤ì¤˜"

print("="*60)
print("Baseline")
print("="*60)
print(baseline_rag_chain.invoke(q1))

print("\n" + "="*60)
print("Agentic")
print("="*60)
print(agentic_react(q1))

# human score

import pandas as pd
import numpy as np

RUBRIC = [
    ("correctness",  "ì›ì¸/ì¡°ì¹˜ê°€ ë…¼ë¦¬ì ìœ¼ë¡œ íƒ€ë‹¹í•œê°€?"),
    ("evidence",     "ë§¤ë‰´ì–¼/ë¡œê·¸ ê·¼ê±°ë¥¼ ì¸ìš©/ì—°ê²°í–ˆëŠ”ê°€? (í™˜ê° ì ì€ê°€)"),
    ("actionability","í˜„ì¥ì—ì„œ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì ˆì°¨/ì²´í¬ê°€ ìˆëŠ”ê°€?"),
    ("safety",       "ê·¼ê±° ì—†ì´ ë‹¨ì •/ìœ„í—˜í•œ ì¡°ì¹˜ë¥¼ ë§í•˜ì§€ ì•ŠëŠ”ê°€?")
]

def make_human_eval_sheet_single(
    question,
    baseline_answer,
    agentic_answer,
    rubric=RUBRIC
):
    row = {
        "case_id": "C001",
        "question": question,
        "log": "",
        "baseline_answer": baseline_answer,
        "agentic_answer": agentic_answer,
        "preference(agentic_vs_baseline)": "agentic",
        "notes": ""
    }

    for key, _ in rubric:
        row[f"baseline_{key}(1-5)"] = 3
        row[f"agentic_{key}(1-5)"]  = 3

    return pd.DataFrame([row])



human_df = make_human_eval_sheet_single(
    q1,
    baseline_rag_chain.invoke(q1),
    agentic_react(q1)
)


print("âœ… Human eval sheet created.")
print("ğŸ‘‰ Fill preference: Win / Lose / Tie")
print("ğŸ‘‰ Fill scores: 1 (ë§¤ìš° ë‚˜ì¨) ~ 5 (ë§¤ìš° ìš°ìˆ˜)")


human_df







