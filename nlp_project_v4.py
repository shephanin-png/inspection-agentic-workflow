# -*- coding: utf-8 -*-
"""NLP_Project_v4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ko1I7GXVEnD7yNWRJ27tqEY9kuUGV3Ag
"""

# 0) 환경 확인
# ----------------------------
!nvidia-smi || true
!cat /etc/os-release | head -n 3
!python --version

# 1) 패키지 설치
#    - langchain / langgraph / chroma / pymupdf
# ----------------------------
!pip -q install -U \
  langchain langchain-core langchain-community \
  langchain-ollama langchain-chroma \
  langgraph \
  pymupdf \
  pydantic \
  datasets ragas

# 2) Ollama 설치 + 서버 기동 + 모델 pull (에러 방지)
# ----------------------------
import os, time, subprocess, json, urllib.request

# zstd 설치 (ollama 설치 스크립트에서 요구할 수 있음)
!sudo apt-get update -y >/dev/null
!sudo apt-get install -y zstd curl >/dev/null

# ollama 설치
!curl -fsSL https://ollama.com/install.sh | sh

def is_ollama_up(url="http://127.0.0.1:11434/api/tags", timeout=2) -> bool:
    try:
        with urllib.request.urlopen(url, timeout=timeout) as resp:
            _ = resp.read(200)
        return True
    except Exception:
        return False

# 서버가 이미 떠있으면 다시 띄우지 않음
if not is_ollama_up():
    # 백그라운드로 실행
    _ = subprocess.Popen(["ollama", "serve"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    time.sleep(2)

# 최종 확인 (여기서 실패하면 이후가 다 안됨)
assert is_ollama_up(), "❌ Ollama server가 떠있지 않습니다. 'ollama serve'가 실패했어요."

# 모델 pull (없으면 자동으로 받음)
!ollama pull llama3
!ollama pull nomic-embed-text

print("✅ Ollama ready")

# 3) Google Drive 마운트 + PDF 로드 (경로/파일 체크 포함)
# ----------------------------
!pip install pypdf
from google.colab import drive
from langchain_community.document_loaders import PyPDFLoader

drive.mount("/content/drive")
pdf_path = "/content/drive/MyDrive/[GW_INSTEK]GPT-9800-SERIES_USER-MANUAL_KR.pdf"
loader = PyPDFLoader(pdf_path)

if not os.path.exists(pdf_path):
    raise FileNotFoundError(
        f"❌ PDF를 못 찾음: {pdf_path}\n"
        "Drive 경로/파일명을 확인하고 PDF_PATH를 수정하세요."
    )


loader = PyPDFLoader(pdf_path)
documents = loader.load()
print("✅ Manual pages:", len(documents))
assert len(documents) > 0, "❌ PDF에서 페이지를 못 읽었습니다. (암호화/손상 가능)"

# 데이터 준비
import pandas as pd
import numpy as np

np.random.seed(42)

# ----- Constants -----
EQPID = "KR-010101"
MODEL = "LGMONYTV"
Program_Version = "260206"
Equipment_Version = "M.KOR"
Spec_Min_mA = 0.80
Spec_Max_mA = 10.00

def make_row(i, label, s6f1_result, saved_result, delta_s, acw):
    return {
        "issue_id": f"{i:03d}",
        "label": label,  # OK / NG (정답 라벨)
        "EQPID": EQPID,
        "SETID": f"500MONY{i:04d}",
        "MODEL": MODEL,
        "Program_Version": Program_Version,
        "Equipment_Version": Equipment_Version,
        "S6F12_ACK": 0,
        "delta_S6F11_S6F12_s": int(delta_s),
        "S6F1_RESULT": s6f1_result,     # 장비 판단
        "ACW_Mea_mA": round(float(acw), 2),
        "Spec_Min_mA": Spec_Min_mA,
        "Spec_Max_mA": Spec_Max_mA,
        "Saved_Result": saved_result,   # 저장 결과
    }

rows = []

# ----- OK 데이터 100개 (001~100): RESULT=OK, Saved=OK -----
ok_acw = np.clip(
    np.random.normal(loc=4.5, scale=2.0, size=100),
    Spec_Min_mA + 0.01, Spec_Max_mA - 0.01
)
ok_delta = np.random.choice([1, 2, 3], size=100, p=[0.6, 0.3, 0.1])

for idx in range(1, 101):
    rows.append(make_row(idx, "OK", "OK", "OK", ok_delta[idx-1], ok_acw[idx-1]))

# ----- NG 데이터 10개 (101~110)
#  - mismatch 6개: S6F1 OK but Saved NG
#  - true NG 4개: S6F1 NG and Saved NG
ng_plan = {
    101: ("NG", "OK", "NG", 1, 3.10),
    102: ("NG", "OK", "NG", 2, 4.62),
    103: ("NG", "NG", "NG", 1, 2.40),
    104: ("NG", "OK", "NG", 3, 9.12),
    105: ("NG", "NG", "NG", 1, 6.07),
    106: ("NG", "OK", "NG", 2, 2.88),
    107: ("NG", "NG", "NG", 1, 1.05),
    108: ("NG", "OK", "NG", 2, 7.41),
    109: ("NG", "NG", "NG", 1, 3.77),
    110: ("NG", "OK", "NG", 2, 5.18),
}

for idx in range(101, 111):
    label, s6f1_result, saved_result, delta_s, acw = ng_plan[idx]
    # 스펙은 정상 찍히는 조건 유지(스펙 범위 내로 클립)
    acw = min(max(acw, Spec_Min_mA + 0.01), Spec_Max_mA - 0.01)
    rows.append(make_row(idx, label, s6f1_result, saved_result, delta_s, acw))

df = pd.DataFrame(rows)

# 파생 컬럼: mismatch 여부 / 케이스 타입
df["is_mismatch"] = ((df["S6F1_RESULT"] != df["Saved_Result"]) & (df["label"] == "NG")).astype(int)
df["case_type"] = np.where(
    df["label"] == "OK",
    "ok_normal",
    np.where(df["is_mismatch"] == 1, "mismatch_ok_saved_ng", "true_ng_s6f1_ng")
)

# ----- CSV 저장 -----
out_path = "/content/inspection_logs_110.csv"
df.to_csv(out_path, index=False, encoding="utf-8-sig")

print("Saved:", out_path)
df.head(10)

# 5) Split + Vectorstore (Chroma) - Ollama Embeddings 사용
# ----------------------------
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=100,
    separators=["\n\n", "\n", " ", ""]
)
splits = splitter.split_documents(documents)
print("✅ chunks:", len(splits))
assert len(splits) > 0, "❌ split 결과가 0입니다."


embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings,
    persist_directory="/content/chroma_gpt9800_manual"
)

print("Vector DB ready:", vectorstore._collection.count())

from langchain_community.chat_models import ChatOllama

# Baseline은 llama3
llm_baseline = ChatOllama(model="llama3", temperature=0.2)

# Agentic은 tools 지원 모델
llm_agent = ChatOllama(model="llama3.1:8b", temperature=0.2)

# 6) Baseline (Vanilla + RAG chain)
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.documents import Document
from typing import List

# 모델 변수 하나로 통일 (에러 방지)
llm_baseline = ChatOllama(model="llama3", temperature=0.2)

# retriever 고정 (k=4)
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 4})

# evidence 번호를 부여해서 "근거를 실제로 인용"하게 만들기
def format_docs(docs: List[Document]) -> str:
    blocks = []
    for i, d in enumerate(docs, 1):
        src = d.metadata.get("source", "manual_pdf")
        page = d.metadata.get("page", None)
        page_str = f"page={page}" if page is not None else "page=N/A"
        content = (d.page_content or "").strip()
        blocks.append(f"[EVIDENCE {i}] [manual] source={src} {page_str}\n{content}")
    return "\n\n---\n\n".join(blocks)

# 핵심: evidence는 반드시 위 EVIDENCE에서 "직접 인용"하도록 강제
baseline_template = """너는 GPT-9804 검사 장비 전문가야.
반드시 아래 MANUAL(EVIDENCE) 내용에 근거해서만 답변해.

중요 규칙:
- 원인/조치/절차는 반드시 아래 EVIDENCE에서 나온 문장을 근거로 제시해야 한다.
- '원인 분석 evidence'에는 반드시 아래 EVIDENCE에서 문장을 그대로 짧게 인용하고,
  끝에 (EVIDENCE 번호, page) 형태로 표시해라.
- EVIDENCE에서 근거를 찾지 못하면:
  ▶ 원인에는 'MANUAL 근거 부족으로 단정 불가(추가 확인 필요)' 라고 쓰고,
  ▶ 원인 분석 evidence에는 '근거 없음'이라고 적어라.
- 절대 OK/NG 같은 판단을 임의로 단정하지 마라.

반드시 아래 형식으로만 출력:
▶ 요약
(1~2줄)

▶ 원인
(근거가 있으면 원인 1~3개, 없으면 '근거 부족' 명시)

▶ 원인 분석 evidence
1) "..." (EVIDENCE n, page=?)
2) "..." (EVIDENCE n, page=?)

▶ 임시 방안
(근거가 없으면 'MANUAL 근거 부족으로 제안 불가(추가 확인 필요)')

▶ 추가 필요 사항
(추가로 필요한 로그/설정/확인 항목)

MANUAL(EVIDENCE):
{context}

QUESTION:
{question}
"""

baseline_prompt = ChatPromptTemplate.from_template(baseline_template)

baseline_rag_chain = (
    {
        "context": retriever | RunnableLambda(format_docs),
        "question": RunnablePassthrough()
    }
    | baseline_prompt
    | llm_baseline
    | StrOutputParser()
)

print("Baseline RAG chain ready (fixed).")

# #7) Agentic (JSON-ReAct)
# ============================================

import json, re
from typing import Dict, Any, List, Optional
from langchain_core.messages import HumanMessage
from langchain_ollama import ChatOllama
from langchain.tools import tool
from langchain.tools import BaseTool

# ---- LLM (agentic) ----
llm_agent = ChatOllama(model="llama3", temperature=0.2)

# ---- Helper: safe JSON extract ----
def extract_json_objects(text: str) -> List[Dict[str, Any]]:
    objs = []
    # NOTE: naive but works for our constrained outputs
    for m in re.finditer(r"\{[\s\S]*?\}", text):
        s = m.group()
        try:
            obj = json.loads(s)
            if isinstance(obj, dict):
                objs.append(obj)
        except Exception:
            continue
    return objs

def extract_final(text: str) -> Optional[str]:
    objs = extract_json_objects(text)
    for obj in reversed(objs):
        if "final" in obj and isinstance(obj["final"], str):
            return obj["final"]
    return None

def extract_tool_call(text: str, tool_names: set) -> Optional[Dict[str, str]]:
    objs = extract_json_objects(text)
    for obj in reversed(objs):
        t = obj.get("tool")
        if t in tool_names:
            inp = obj.get("input", "")
            if not isinstance(inp, str):
                inp = json.dumps(inp, ensure_ascii=False)
            return {"tool": t, "input": inp}
    return None

# ---- Tool 1: Manual retrieval (WITH evidence labels + page) ----
@tool
def manual_retrieve_tool(query: str) -> str:
    """매뉴얼에서 관련 chunk만 가져와 반환 (답변 생성 X). 반드시 EVIDENCE 번호/페이지 포함."""
    docs = retriever.invoke(query)
    if not docs:
        return "NO_CONTEXT_FOUND"

    blocks = []
    for i, d in enumerate(docs, 1):
        page = d.metadata.get("page", "N/A")
        src = d.metadata.get("source", "manual_pdf")
        txt = (d.page_content or "").strip()
        if len(txt) > 1500:
            txt = txt[:1500] + "\n...(truncated)"
        blocks.append(f"[EVIDENCE {i}] [manual] source={src} page={page}\n{txt}")
    return "\n\n---\n\n".join(blocks)

# ---- Tool 2: Inspection log summary from df ----
@tool
def inspection_log_tool(_: str = "") -> str:
    """df 기반 mismatch/NG 요약"""
    total = len(df)
    ng = int((df["label"] == "NG").sum()) if "label" in df.columns else 0
    mismatch = int(df["is_mismatch"].sum()) if "is_mismatch" in df.columns else 0
    cols = [c for c in ["issue_id", "SETID", "S6F1_RESULT", "Saved_Result", "ACW_Mea_mA"] if c in df.columns]
    sample = df[df.get("is_mismatch", 0) == 1][cols].head(5) if "is_mismatch" in df.columns else df.head(0)
    return f"""
[Inspection Summary]
- Total: {total}
- NG: {ng}
- Mismatch: {mismatch}

[Sample mismatch rows]
{sample.to_string(index=False)}
""".strip()

# ---- Tool 3: Extract signals from provided log text (JSON string) ----
@tool
def extract_log_signals_tool(text: str) -> str:
    """로그 텍스트에서 ack / SxFy / LO/HI SET 힌트 추출(문자열 JSON으로 반환)"""
    ack = re.findall(r"\back\s*[:=]?\s*(\d+)\b", text, flags=re.I)
    secs = re.findall(r"\bS\d+F\d+\b", text, flags=re.I)
    params = re.findall(r"\b(LO\s*SET|HI\s*SET)\b", text, flags=re.I)
    obj = {
        "ack": int(ack[-1]) if ack else None,
        "secs_messages": sorted(list(set([s.upper() for s in secs]))) if secs else [],
        "params": sorted(list(set([p.upper().replace(" ", "") for p in params]))) if params else [],
        "log_len": len(text),
    }
    return json.dumps(obj, ensure_ascii=False)

TOOLS: Dict[str, BaseTool] = {
    "manual_retrieve_tool": manual_retrieve_tool,
    "inspection_log_tool": inspection_log_tool,
    "extract_log_signals_tool": extract_log_signals_tool,
}
TOOL_NAMES = set(TOOLS.keys())

# ============================================
# Agentic JSON-ReAct loop
# ============================================

def agentic_react(question: str, log_snippet: str = "", max_steps: int = 6) -> str:
    """
    Agentic 입력: (질문 + 로그 snippet optional)
    - LLM이 JSON으로 tool call을 제안하면 우리가 실행하고 memory에 누적
    - READY면 final answer 생성
    """
    memory = ""
    # allow multiple manual retrievals; only block exact same call repeatedly
    used_calls = set()

    def final_answer() -> str:
        final_prompt = f"""
너는 GPT-9800 검사 장비 트러블슈팅 전문가다.
아래 OBSERVATIONS(매뉴얼 발췌/로그 요약/검사이력)에 근거해서만 답하라.
OBSERVATIONS에 근거가 없으면 반드시 'MANUAL 근거 부족(추가 확인 필요)'로 단정 금지.

중요 규칙:
- '원인 분석 evidence'에는 OBSERVATIONS의 [EVIDENCE n] 문장을 그대로 짧게 인용하고
  끝에 (EVIDENCE n, page=?)를 반드시 표기하라.
- 인용 가능한 EVIDENCE가 없으면 '근거 없음'이라고 명시하라.

OBSERVATIONS:
{memory}

USER QUESTION:
{question}

LOG SNIPPET (may be empty):
{log_snippet}

반드시 아래 형식으로만 출력:
▶ 요약
(1~2줄)

▶ 원인
(가능한 원인 TOP3, 근거가 없으면 'MANUAL 근거 부족' 명시)

▶ 원인 분석 evidence
1) "..." (EVIDENCE n, page=?)
2) "..." (EVIDENCE n, page=?)

▶ 임시 방안
(근거가 없으면 'MANUAL 근거 부족으로 제안 불가(추가 확인 필요)')

▶ 추가 필요 사항
(추가로 필요한 로그/설정/확인 항목)
"""
        return llm_agent.invoke([HumanMessage(content=final_prompt)]).content

    for step in range(max_steps):
        step_prompt = f"""
너는 GPT-9800 트러블슈팅 에이전트다.
너는 아래 도구를 JSON 형식으로 호출할 수 있다:

- extract_log_signals_tool: 로그 텍스트에서 ACK/SxFy/LO/HI SET 힌트 추출
- manual_retrieve_tool: 추출 힌트/질문 키워드로 매뉴얼 근거(EVIDENCE)를 검색
- inspection_log_tool: df 기반 mismatch/NG 통계 요약

진행 원칙:
- 로그 snippet이 제공되었으면 extract_log_signals_tool를 먼저 고려하라.
- 매뉴얼 근거가 부족하면 manual_retrieve_tool를 다른 쿼리로 2~3회까지 재시도해도 된다.
- mismatch 가능성이 있거나 'Saved_Result vs S6F1_RESULT' 같은 이슈면 inspection_log_tool 사용을 고려하라.
- 근거가 충분해졌다고 판단되면 {{\"final\":\"READY\"}} 를 출력해라.

OBSERVATIONS:
{memory}

USER QUESTION:
{question}

LOG SNIPPET:
{log_snippet}

가능한 출력(JSON 하나만):
{{"tool":"extract_log_signals_tool","input":"<log snippet text>"}}
{{"tool":"manual_retrieve_tool","input":"<query for manual search>"}}
{{"tool":"inspection_log_tool","input":""}}
{{"final":"READY"}}
"""
        resp = llm_agent.invoke([HumanMessage(content=step_prompt)]).content

        fin = extract_final(resp)
        if fin is not None and fin.strip().upper() == "READY":
            return final_answer()

        call = extract_tool_call(resp, TOOL_NAMES)
        if call is None:
            # if model didn't follow format, just finalize (stable)
            return final_answer()

        tname, tin = call["tool"], call["input"]
        call_sig = (tname, tin.strip())
        if call_sig in used_calls:
            # repeated identical call -> stop looping
            return final_answer()
        used_calls.add(call_sig)

        # execute tool
        obs = TOOLS[tname].invoke(tin)
        memory += f"\n[{tname} input={tin}]\n{obs}\n"

    return final_answer()

print("Agentic(JSON-ReAct) ready (fixed).")

# 8) 비교 실행 (Baseline vs Agentic)
# =========================
q1 = "S6F12 ACK=1 발생으로 NG 가 발생했어. 발생 원인 및 조치 방안 알려줘"

print("="*60)
print("Baseline")
print("="*60)
print(baseline_rag_chain.invoke(q1))

print("\n" + "="*60)
print("Agentic")
print("="*60)
print(agentic_react(q1))

# 9) RAGAS 평가 (Baseline / Agentic 각각)
# =========================================

from typing import List, Dict, Any
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper
from langchain_ollama import ChatOllama, OllamaEmbeddings


# -------------------------
# 0) 샘플 준비
# -------------------------
# eval_samples = [...]

eval_samples = [{
        "case_id": "M001",
        "question": "GPT-9000 시리즈 모델별로 지원하는 테스트 항목(ACW/DCW/IR/GB)은 어떻게 다른가?",
        "ground_truth": "GPT-9801은 ACW, GPT-9802는 ACW/DCW, GPT-9803은 ACW/DCW/IR, GPT-9804는 ACW/DCW/IR/GB를 지원한다."
    },
    {
        "case_id": "M002",
        "question": "장비 전원을 켜기 전에 후면에서 확인해야 하는 항목과 전원 코드/접지 관련 주의사항은 무엇인가?",
        "ground_truth": "전원 켜기 전 후면 패널의 라인 전압 셀렉터가 사용하는 라인 전압과 동일한지 확인하고, 전원 코드를 AC 입력 소켓에 연결한다. 전원 코드에 대지(Earth) 접지선이 없으면 장비의 GND 단자와 대지 접지를 반드시 연결해야 한다."
    },
    {
        "case_id": "M003",
        "question": "장비를 켰을 때 LED 점등 및 셀프 테스트 후 장비는 어떤 상태로 전환되는가?",
        "ground_truth": "전원이 켜지면 모든 LED 표시등이 점등되며, 시스템 셀프 테스트를 에러 없이 통과한 뒤 장비는 보기(VIEW) 상태로 전환되어 동작 준비 상태가 된다."
    },
    {
        "case_id": "M004",
        "question": "ACW/DCW/IR 테스트에서 테스트 리드선(GHT-114)은 어떤 단자에 어떻게 연결해야 하는가?",
        "ground_truth": "ACW/DCW/IR 테스트 시 고전압 테스트 리드선(적색)은 HIGH VOLTAGE 단자에 연결하고, 리턴 테스트 리드선(백색)은 RETURN 단자에 연결하며 보호 막대를 고정한다."
    },
    {
        "case_id": "M005",
        "question": "GB(그라운드 본드) 테스트에서 테스트 리드선(GTL-115)은 어떤 단자에 연결해야 하는가?",
        "ground_truth": "GB 테스트 시 Sense H/L 리드선은 SENSE H/L 단자에 연결하고, Source H/L 리드선은 SOURCE H/L 단자에 연결한다."
    }]


# eval_samples 형태
if isinstance(eval_samples, str):
    # 문자열 하나면 1개 샘플로 감쌈
    eval_samples = [{"question": eval_samples, "log": "", "ground_truth": ""}]
elif isinstance(eval_samples, list):
    if len(eval_samples) == 0:
        raise ValueError("eval_samples가 비어있음")
    # list[str]면 question만 있는 샘플로 변환
    if isinstance(eval_samples[0], str):
        eval_samples = [{"question": q, "log": "", "ground_truth": ""} for q in eval_samples]
    # list[dict]가 아니면 에러
    elif not isinstance(eval_samples[0], dict):
        raise ValueError(f"eval_samples 원소 타입이 dict가 아님: {type(eval_samples[0])}")

# dict인데 list가 아닌 경우도 대비
elif isinstance(eval_samples, dict):
    eval_samples = [eval_samples]
else:
    raise ValueError(f"eval_samples 타입이 이상함: {type(eval_samples)}")

print("✅ eval_samples normalized. n =", len(eval_samples))
print("sample keys:", eval_samples[0].keys())




# -------------------------
# 1) Baseline/Agent 실행 wrapper
# -------------------------
def run_baseline(q: str, log_text: str) -> str:

    return baseline_rag_chain.invoke(q + "\n\n[LOG]\n" + (log_text or ""))

def run_agent(q: str, log_text: str) -> str:
    if "agentic_react" in globals():
        try:
            if "log_snippet" in agentic_react.__code__.co_varnames:
                return agentic_react(q, log_snippet=log_text or "")
            else:
                return agentic_react(q)
        except Exception as e:
            return f"[AGENTIC_ERROR] {str(e)[:200]}"
    else:
        return "[AGENTIC_ERROR] agentic_answer/agentic_react 함수가 없습니다."

# -------------------------
# 2) contexts 구성 (retriever 결과를 ragas용 list-of-chunks로)
# -------------------------
def get_contexts_for_ragas(q: str, log_text: str, k: int = 4, max_chars_each: int = 1800) -> List[str]:
    query = q + "\n" + (log_text or "")
    docs = retriever.invoke(query)
    ctxs = []
    for d in docs[:k]:
        page = d.metadata.get("page", "N/A")
        src = d.metadata.get("source", "manual_pdf")
        txt = (d.page_content or "").strip()
        if len(txt) > max_chars_each:
            txt = txt[:max_chars_each] + "\n...(truncated)"
        ctxs.append(f"[source={src} page={page}]\n{txt}")
    return ctxs

# -------------------------
# 3) 평가 데이터 생성
# -------------------------
baseline_answers = []
agent_answers = []
contexts = []
questions = []

print("\n### 평가용 데이터 생성 중... ###")
for s in eval_samples:
    q = s["question"]
    lg = s.get("log", "")
    gt = s.get("ground_truth", "")

    # ragas는 question을 string으로 받음
    questions.append(q + "\n\n[LOG]\n" + (lg or ""))


    # contexts: ragas 포맷은 List[List[str]]
    contexts.append(get_contexts_for_ragas(q, lg, k=4))

    # answers
    baseline_answers.append(run_baseline(q, lg))
    agent_answers.append(run_agent(q, lg))

print("✅ Eval samples:", len(eval_samples))

# -------------------------
# 4) RAGAS judge LLM / embeddings 준비
# -------------------------
if "ragas_llm" not in globals():
    judge_llm = ChatOllama(model="llama3", temperature=0.0)
    ragas_llm = LangchainLLMWrapper(judge_llm)

if "ragas_emb" not in globals():
    judge_emb = OllamaEmbeddings(model="nomic-embed-text")
    ragas_emb = LangchainEmbeddingsWrapper(judge_emb)

# -------------------------
# 5) RAGAS 평가 함수
# -------------------------
def ragas_eval(answers: List[str], label: str):
    data = {
        "question": questions,
        "answer": answers,
        "contexts": contexts,
    }



    ds = Dataset.from_dict(data)

    print(f"\n===== RAGAS EVAL: {label} =====")
    results = evaluate(
        dataset=ds,
        metrics=[faithfulness, answer_relevancy],
        llm=ragas_llm,
        embeddings=ragas_emb
    )
    print(results)
    return results.to_pandas()

df_base = ragas_eval(baseline_answers, "Baseline")
df_agent = ragas_eval(agent_answers, "Agentic")

print("\n✅ Baseline scores DF shape:", df_base.shape)
print("✅ Agentic scores DF shape:", df_agent.shape)

# -------------------------
# 6) 비교 요약 (평균 + 차이)
# -------------------------
def summarize_ragas(df_base: pd.DataFrame, df_agent: pd.DataFrame) -> pd.DataFrame:

    base_num_cols = [c for c in df_base.columns if pd.api.types.is_numeric_dtype(df_base[c])]
    agent_num_cols = [c for c in df_agent.columns if pd.api.types.is_numeric_dtype(df_agent[c])]
    cols = [c for c in base_num_cols if c in agent_num_cols]

    summary = pd.DataFrame({
        "metric": cols,
        "baseline_mean": [float(df_base[c].mean()) for c in cols],
        "agentic_mean": [float(df_agent[c].mean()) for c in cols],
        "delta(agentic-baseline)": [float(df_agent[c].mean() - df_base[c].mean()) for c in cols],
    })
    return summary

ragas_summary = summarize_ragas(df_base, df_agent)
print("\n===== RAGAS SUMMARY (MEAN) =====")
print(ragas_summary)



